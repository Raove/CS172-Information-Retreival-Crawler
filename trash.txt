# #followed a tutorial online https://dev.to/fprime/how-to-create-a-web-crawler-from-scratch-in-python-2p46
        class PyCrawler(object):
            def __init__(self, starting_url, hops):
                self.count = 0
                self.levels = hops
                self.starting_url = starting_url
                self.visited = set()

            def get_html(self, url):
                try:
                    html = requests.get(url)
                except Exception as e:
                    print(e)
                    return ""
                return html.content.decode('latin-1') 

            def get_links(self, url):    
                html = self.get_html(url)    
                parsed = urlparse(url)    
                base = f"{parsed.scheme}://{parsed.netloc}"    
                links = re.findall('''<a\s+(?:[^>]*?\s+)?href="([^"]*)"''', html)    
                for i, link in enumerate(links):    
                    if not urlparse(link).netloc:    
                        link_with_base = base + link    
                        links[i] = link_with_base       

                return set(filter(lambda x: 'mailto' not in x, links))    

            def extract_info(self, url):    
                html = self.get_html(url)  
                return html  
                # title = re.findall("<title>.*?</title>", html)    
                # meta = re.findall("<meta .*?name=[\"'](.*?)['\"].*?content=[\"'](.*?)['\"].*?>", html)    
                # return dict(meta)             

            def crawl(self, url):           
                for link in self.get_links(url):    
                    if link in self.visited:        
                        continue                    
                    print(link)                 
                    self.visited.add(link)            
                    data = self.extract_info(link) 
                    # print(self.count)  
                    print(self.levels)
                    # if self.count == self.levels:
                    #     print("STOP HERE BITCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH")
                    # self.count = self.count + 1   
                    f = open(r"C:\Users\raoul\Documents\GitHub\CS172-Information-Retreival-Crawler\files\page" + str(self.count) + ".html", "w")
                    f.write(data)
                    f.close()
                    self.crawl(link)    

            def start(self):                  
                self.crawl(self.starting_url)

        if __name__ == "__main__":   
            crawler = PyCrawler("https://google.com", levels)
            crawler.start()